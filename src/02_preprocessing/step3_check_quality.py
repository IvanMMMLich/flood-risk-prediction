"""
===============================================================================
                    STEP 3: DATA QUALITY CHECK & CLEANING
                    
                    –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –æ—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
===============================================================================

–¶–ï–õ–¨:
-----
1. –ó–∞–º–µ–Ω–∏—Ç—å –∞–Ω–æ–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (123)
2. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø—Ä–æ–ø—É—Å–∫–∏ –∏ –¥—É–±–ª–∏–∫–∞—Ç—ã
3. –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤—ã–±—Ä–æ—Å—ã
4. –õ–æ–≥–∞—Ä–∏—Ñ–º–∏—Ä–æ–≤–∞—Ç—å —Å–∫–æ—à–µ–Ω–Ω—ã–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
5. –ó–∞–ø—É—Å—Ç–∏—Ç—å baseline v1 –Ω–∞ –æ—á–∏—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö

–û–ñ–ò–î–ê–ï–ú–´–ô –†–ï–ó–£–õ–¨–¢–ê–¢:
-------------------
- –ß–∏—Å—Ç—ã–π –¥–∞—Ç–∞—Å–µ—Ç –±–µ–∑ –∞–Ω–æ–º–∞–ª–∏–π
- –£–ª—É—á—à–µ–Ω–∏–µ baseline –Ω–∞ ~0.005-0.010
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import warnings
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix
import pickle

warnings.filterwarnings('ignore')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—É—Ç–µ–π
ROOT_DIR = Path(__file__).parent.parent.parent
DATA_RAW = ROOT_DIR / 'data' / 'raw'
DATA_PROCESSED = ROOT_DIR / 'data' / 'processed'
RESULTS = ROOT_DIR / 'results' / 'step3_cleaning'
MODEL_V1 = ROOT_DIR / 'results' / 'model_versions' / 'v1_after_cleaning'

# –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫–∏
RESULTS.mkdir(parents=True, exist_ok=True)
MODEL_V1.mkdir(parents=True, exist_ok=True)
DATA_PROCESSED.mkdir(parents=True, exist_ok=True)

def load_data():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö."""
    print("\n" + "="*60)
    print("STEP 3: DATA QUALITY CHECK & CLEANING")
    print("="*60)
    
    train_df = pd.read_csv(DATA_RAW / 'train.csv')
    test_df = pd.read_csv(DATA_RAW / 'test.csv')
    
    print(f"\nüìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ:")
    print(f"   Train: {len(train_df)} –∑–∞–ø–∏—Å–µ–π")
    print(f"   Test: {len(test_df)} –∑–∞–ø–∏—Å–µ–π")
    
    return train_df, test_df

def check_anomalies(df, df_name="DataFrame"):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∞–Ω–æ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π."""
    print(f"\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞–Ω–æ–º–∞–ª–∏–π –≤ {df_name}:")
    print("-"*40)
    
    anomalies = {}
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º 123 –≤ –≤–æ–∑—Ä–∞—Å—Ç–µ
    age_123 = (df['person_age'] == 123).sum()
    if age_123 > 0:
        print(f"   ‚ö†Ô∏è person_age = 123: {age_123} –∑–∞–ø–∏—Å–µ–π")
        anomalies['person_age'] = age_123
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º 123 –≤ —Å—Ç–∞–∂–µ
    emp_123 = (df['person_emp_length'] == 123).sum()
    if emp_123 > 0:
        print(f"   ‚ö†Ô∏è person_emp_length = 123: {emp_123} –∑–∞–ø–∏—Å–µ–π")
        anomalies['person_emp_length'] = emp_123
    
    if not anomalies:
        print("   ‚úÖ –ê–Ω–æ–º–∞–ª–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
    
    return anomalies

def fix_anomalies(df):
    """–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π."""
    print("\nüîß –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π:")
    print("-"*40)
    
    df_clean = df.copy()
    
    # –ó–∞–º–µ–Ω—è–µ–º 123 –Ω–∞ –º–µ–¥–∏–∞–Ω—É –¥–ª—è –≤–æ–∑—Ä–∞—Å—Ç–∞
    if (df_clean['person_age'] == 123).any():
        median_age = df_clean[df_clean['person_age'] != 123]['person_age'].median()
        count_age = (df_clean['person_age'] == 123).sum()
        df_clean.loc[df_clean['person_age'] == 123, 'person_age'] = median_age
        print(f"   ‚úÖ person_age: –∑–∞–º–µ–Ω–µ–Ω–æ {count_age} –∑–Ω–∞—á–µ–Ω–∏–π –Ω–∞ –º–µ–¥–∏–∞–Ω—É {median_age:.0f}")
    
    # –ó–∞–º–µ–Ω—è–µ–º 123 –Ω–∞ –º–µ–¥–∏–∞–Ω—É –¥–ª—è —Å—Ç–∞–∂–∞
    if (df_clean['person_emp_length'] == 123).any():
        median_emp = df_clean[df_clean['person_emp_length'] != 123]['person_emp_length'].median()
        count_emp = (df_clean['person_emp_length'] == 123).sum()
        df_clean.loc[df_clean['person_emp_length'] == 123, 'person_emp_length'] = median_emp
        print(f"   ‚úÖ person_emp_length: –∑–∞–º–µ–Ω–µ–Ω–æ {count_emp} –∑–Ω–∞—á–µ–Ω–∏–π –Ω–∞ –º–µ–¥–∏–∞–Ω—É {median_emp:.0f}")
    
    return df_clean

def check_missing_values(df):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π."""
    print("\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤:")
    print("-"*40)
    
    missing = df.isnull().sum()
    missing_pct = (df.isnull().sum() / len(df)) * 100
    
    if missing.sum() == 0:
        print("   ‚úÖ –ü—Ä–æ–ø—É—Å–∫–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
    else:
        missing_df = pd.DataFrame({
            'Missing': missing[missing > 0],
            'Percent': missing_pct[missing > 0]
        }).sort_values('Percent', ascending=False)
        print(missing_df)
    
    return missing

def check_duplicates(df):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤."""
    print("\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤:")
    print("-"*40)
    
    # –ü–æ–ª–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã
    full_duplicates = df.duplicated().sum()
    print(f"   –ü–æ–ª–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {full_duplicates}")
    
    # –î—É–±–ª–∏–∫–∞—Ç—ã –ø–æ ID (–µ—Å–ª–∏ –µ—Å—Ç—å)
    if 'id' in df.columns:
        id_duplicates = df['id'].duplicated().sum()
        print(f"   –î—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ ID: {id_duplicates}")
    
    if full_duplicates == 0:
        print("   ‚úÖ –î—É–±–ª–∏–∫–∞—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
    
    return full_duplicates

def handle_skewness(df):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–∫–æ—à–µ–Ω–Ω—ã—Ö —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π."""
    print("\nüìä –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–∫–æ—à–µ–Ω–Ω–æ—Å—Ç–∏:")
    print("-"*40)
    
    df_clean = df.copy()
    
    # –õ–æ–≥–∞—Ä–∏—Ñ–º–∏—Ä—É–µ–º person_income
    skew_before = df_clean['person_income'].skew()
    df_clean['person_income_log'] = np.log1p(df_clean['person_income'])
    skew_after = df_clean['person_income_log'].skew()
    
    print(f"   person_income skewness:")
    print(f"   –î–æ: {skew_before:.2f}")
    print(f"   –ü–æ—Å–ª–µ log: {skew_after:.2f} ‚úÖ")
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    fig, axes = plt.subplots(1, 2, figsize=(12, 4))
    
    # –î–æ
    axes[0].hist(df['person_income'], bins=50, edgecolor='black', alpha=0.7)
    axes[0].set_title(f'Original Income (skew={skew_before:.2f})')
    axes[0].set_xlabel('Income')
    axes[0].set_ylabel('Frequency')
    
    # –ü–æ—Å–ª–µ
    axes[1].hist(df_clean['person_income_log'], bins=50, edgecolor='black', alpha=0.7)
    axes[1].set_title(f'Log Income (skew={skew_after:.2f})')
    axes[1].set_xlabel('Log(Income+1)')
    axes[1].set_ylabel('Frequency')
    
    plt.tight_layout()
    plt.savefig(RESULTS / 'income_transformation.png', dpi=100, bbox_inches='tight')
    plt.show()
    
    return df_clean

def analyze_outliers(df):
    """–ê–Ω–∞–ª–∏–∑ –≤—ã–±—Ä–æ—Å–æ–≤."""
    print("\nüìä –ê–Ω–∞–ª–∏–∑ –≤—ã–±—Ä–æ—Å–æ–≤ (IQR –º–µ—Ç–æ–¥):")
    print("-"*40)
    
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    outliers_summary = {}
    
    for col in numeric_cols:
        if col not in ['id', 'loan_status']:
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            outliers = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()
            outliers_pct = (outliers / len(df)) * 100
            
            if outliers > 0:
                outliers_summary[col] = {
                    'count': outliers,
                    'percent': outliers_pct,
                    'lower_bound': lower_bound,
                    'upper_bound': upper_bound
                }
                print(f"   {col}: {outliers} ({outliers_pct:.1f}%)")
    
    return outliers_summary

def run_baseline_v1(train_df_clean):
    """–ó–∞–ø—É—Å–∫ baseline –Ω–∞ –æ—á–∏—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö."""
    print("\n" + "="*60)
    print("BASELINE MODEL v1 - After Cleaning")
    print("="*60)
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö (–∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ baseline_model.py)
    X = train_df_clean.drop(['id', 'loan_status'], axis=1)
    y = train_df_clean['loan_status']
    
    # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö
    le_dict = {}
    categorical_cols = X.select_dtypes(include=['object']).columns
    
    for col in categorical_cols:
        le = LabelEncoder()
        X.loc[:, col] = le.fit_transform(X[col])
        le_dict[col] = le
    
    # Train/Val split
    X_train, X_val, y_train, y_val = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    # –û–±—É—á–µ–Ω–∏–µ
    model = LogisticRegression(
        class_weight='balanced',
        max_iter=1000,
        random_state=42
    )
    model.fit(X_train, y_train)
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    y_pred_proba = model.predict_proba(X_val)[:, 1]
    y_pred = model.predict(X_val)
    
    # –ú–µ—Ç—Ä–∏–∫–∏
    roc_auc = roc_auc_score(y_val, y_pred_proba)
    
    print(f"\nüìà –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
    print(f"   ROC-AUC: {roc_auc:.4f}")
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å baseline
    baseline_auc = 0.8740  # –ò–∑ v0
    improvement = roc_auc - baseline_auc
    print(f"\nüìä –°–†–ê–í–ù–ï–ù–ò–ï –° BASELINE:")
    print(f"   v0_baseline: {baseline_auc:.4f}")
    print(f"   v1_cleaned:  {roc_auc:.4f}")
    print(f"   –£–ª—É—á—à–µ–Ω–∏–µ:   {improvement:+.4f} {'‚úÖ' if improvement > 0 else '‚ùå'}")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
    with open(MODEL_V1 / 'metrics.txt', 'w') as f:
        f.write("BASELINE MODEL v1 - After Cleaning\n")
        f.write("="*50 + "\n")
        f.write(f"ROC-AUC: {roc_auc:.4f}\n")
        f.write(f"Improvement from v0: {improvement:+.4f}\n")
        f.write("\nChanges applied:\n")
        f.write("- Replaced 123 anomalies with median\n")
        f.write("- Log transformation of person_income\n")
        f.write("- No outliers removed (kept for subprime logic)\n")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    with open(MODEL_V1 / 'model.pkl', 'wb') as f:
        pickle.dump(model, f)
    
    return roc_auc, improvement

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    train_df, test_df = load_data()
    
    # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞–Ω–æ–º–∞–ª–∏–π
    train_anomalies = check_anomalies(train_df, "Train")
    test_anomalies = check_anomalies(test_df, "Test")
    
    # 3. –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π
    train_df_clean = fix_anomalies(train_df)
    test_df_clean = fix_anomalies(test_df)
    
    # 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤
    check_missing_values(train_df_clean)
    
    # 5. –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    check_duplicates(train_df_clean)
    
    # 6. –ê–Ω–∞–ª–∏–∑ –≤—ã–±—Ä–æ—Å–æ–≤
    outliers = analyze_outliers(train_df_clean)
    
    # 7. –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–∫–æ—à–µ–Ω–Ω–æ—Å—Ç–∏
    train_df_clean = handle_skewness(train_df_clean)
    test_df_clean = handle_skewness(test_df_clean)
    
    # 8. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—á–∏—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    train_df_clean.to_csv(DATA_PROCESSED / 'train_cleaned.csv', index=False)
    test_df_clean.to_csv(DATA_PROCESSED / 'test_cleaned.csv', index=False)
    print(f"\nüíæ –û—á–∏—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {DATA_PROCESSED}")
    
    # 9. –ó–∞–ø—É—Å–∫ baseline v1
    roc_auc, improvement = run_baseline_v1(train_df_clean)
    
    # 10. –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
    print("\n" + "="*60)
    print("STEP 3 COMPLETED!")
    print("="*60)
    print(f"‚úÖ –ê–Ω–æ–º–∞–ª–∏–∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω—ã")
    print(f"‚úÖ –°–∫–æ—à–µ–Ω–Ω–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞")
    print(f"‚úÖ Baseline v1: ROC-AUC = {roc_auc:.4f} ({improvement:+.4f})")
    print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {MODEL_V1}")

if __name__ == "__main__":
    main()