"""
===============================================================================
                    STEP 7: FINAL MODEL & SUBMISSION
                    
                    –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –∏ —Å–æ–∑–¥–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏
===============================================================================

–¶–ï–õ–¨:
-----
1. –û–±—É—á–∏—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –Ω–∞ –í–°–ï–• train –¥–∞–Ω–Ω—ã—Ö
2. –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å XGBoost –¥–ª—è –µ—â–µ –ª—É—á—à–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
3. –ü–æ–¥–æ–±—Ä–∞—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
4. –°–æ–∑–¥–∞—Ç—å submission —Ñ–∞–π–ª –¥–ª—è Kaggle
5. –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å

–û–ñ–ò–î–ê–ï–ú–´–ô –†–ï–ó–£–õ–¨–¢–ê–¢:
-------------------
- –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å ROC-AUC > 0.93
- submission.csv –≥–æ—Ç–æ–≤—ã–π –∫ –æ—Ç–ø—Ä–∞–≤–∫–µ
- –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –º–æ–¥–µ–ª–∏
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import warnings
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix
import xgboost as xgb 
import pickle
from datetime import datetime

warnings.filterwarnings('ignore')

# –ü—É—Ç–∏
ROOT_DIR = Path(__file__).parent.parent.parent
DATA_ENGINEERED = ROOT_DIR / 'data' / 'engineered'
DATA_RAW = ROOT_DIR / 'data' / 'raw'
RESULTS = ROOT_DIR / 'results' / 'final_model'
MODEL_V3 = ROOT_DIR / 'results' / 'model_versions' / 'v3_final'
SUBMISSIONS = ROOT_DIR / 'submissions'

# –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫–∏
RESULTS.mkdir(parents=True, exist_ok=True)
MODEL_V3.mkdir(parents=True, exist_ok=True)
SUBMISSIONS.mkdir(parents=True, exist_ok=True)

def load_data():
    """–ó–∞–≥—Ä—É–∑–∫–∞ engineered –¥–∞–Ω–Ω—ã—Ö."""
    print("\n" + "="*60)
    print("STEP 7: FINAL MODEL & SUBMISSION")
    print("="*60)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    train_df = pd.read_csv(DATA_ENGINEERED / 'train_engineered.csv')
    test_df = pd.read_csv(DATA_ENGINEERED / 'test_engineered.csv')
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π test –¥–ª—è ID
    test_original = pd.read_csv(DATA_RAW / 'test.csv')
    
    print(f"\nüìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ:")
    print(f"   Train: {len(train_df)} –∑–∞–ø–∏—Å–µ–π, {len(train_df.columns)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    print(f"   Test: {len(test_df)} –∑–∞–ø–∏—Å–µ–π")
    
    return train_df, test_df, test_original['id']

def train_random_forest_optimized(X_train, y_train, X_val, y_val):
    """–û–±—É—á–µ–Ω–∏–µ Random Forest —Å –ø–æ–¥–±–æ—Ä–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤."""
    print("\nüå≤ RANDOM FOREST —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π:")
    print("-"*40)
    
    # –°–Ω–∞—á–∞–ª–∞ –±—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ –ª—É—á—à–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    print("–ü–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (–∑–∞–π–º–µ—Ç 1-2 –º–∏–Ω—É—Ç—ã)...")
    
    param_grid = {
        'n_estimators': [100, 200],
        'max_depth': [10, 15, 20],
        'min_samples_split': [10, 20],
        'min_samples_leaf': [5, 10]
    }
    
    rf = RandomForestClassifier(
        class_weight='balanced',
        random_state=42,
        n_jobs=-1
    )
    
    grid_search = GridSearchCV(
        rf, 
        param_grid, 
        cv=3, 
        scoring='roc_auc',
        verbose=1,
        n_jobs=-1
    )
    
    grid_search.fit(X_train, y_train)
    
    print(f"\n‚úÖ –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:")
    for param, value in grid_search.best_params_.items():
        print(f"   {param}: {value}")
    
    # –û–±—É—á–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å —Å –ª—É—á—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    best_rf = grid_search.best_estimator_
    
    # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    y_pred_proba = best_rf.predict_proba(X_val)[:, 1]
    rf_auc = roc_auc_score(y_val, y_pred_proba)
    
    print(f"\nüìä Random Forest ROC-AUC: {rf_auc:.4f}")
    
    return best_rf, rf_auc

def train_xgboost(X_train, y_train, X_val, y_val):
    """–û–±—É—á–µ–Ω–∏–µ XGBoost."""
    print("\nüöÄ XGBOOST –º–æ–¥–µ–ª—å:")
    print("-"*40)
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ DMatrix –¥–ª—è XGBoost
    dtrain = xgb.DMatrix(X_train, label=y_train)
    dval = xgb.DMatrix(X_val, label=y_val)
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã XGBoost
    params = {
        'objective': 'binary:logistic',
        'eval_metric': 'auc',
        'max_depth': 6,
        'learning_rate': 0.1,
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'seed': 42,
        'scale_pos_weight': (y_train == 0).sum() / (y_train == 1).sum()  # –¥–ª—è –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞
    }
    
    # –û–±—É—á–µ–Ω–∏–µ —Å early stopping
    evals = [(dtrain, 'train'), (dval, 'val')]
    
    print("–û–±—É—á–µ–Ω–∏–µ XGBoost...")
    xgb_model = xgb.train(
        params,
        dtrain,
        num_boost_round=1000,
        evals=evals,
        early_stopping_rounds=50,
        verbose_eval=100
    )
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
    y_pred_proba = xgb_model.predict(dval)
    xgb_auc = roc_auc_score(y_val, y_pred_proba)
    
    print(f"\nüìä XGBoost ROC-AUC: {xgb_auc:.4f}")
    
    return xgb_model, xgb_auc

def train_final_model(X_train_full, y_train_full):
    """–û–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –í–°–ï–• –¥–∞–Ω–Ω—ã—Ö."""
    print("\n" + "="*60)
    print("–§–ò–ù–ê–õ–¨–ù–ê–Ø –ú–û–î–ï–õ–¨ (–Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö)")
    print("="*60)
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –ø–æ–∏—Å–∫–∞
    final_model = RandomForestClassifier(
        n_estimators=200,
        max_depth=15,
        min_samples_split=10,
        min_samples_leaf=5,
        class_weight='balanced',
        random_state=42,
        n_jobs=-1
    )
    
    print("\nüéØ –û–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –í–°–ï–• train –¥–∞–Ω–Ω—ã—Ö...")
    final_model.fit(X_train_full, y_train_full)
    
    # Cross-validation –¥–ª—è –æ—Ü–µ–Ω–∫–∏
    print("\nCross-validation (5 folds)...")
    cv_scores = cross_val_score(
        final_model, X_train_full, y_train_full, 
        cv=5, scoring='roc_auc', n_jobs=-1
    )
    
    print(f"\nCross-validation ROC-AUC:")
    for i, score in enumerate(cv_scores, 1):
        print(f"   Fold {i}: {score:.4f}")
    print(f"   –°—Ä–µ–¥–Ω–µ–µ: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}")
    
    return final_model, cv_scores.mean()

def analyze_feature_importance(model, feature_names):
    """–ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏."""
    print("\nüìä –í–ê–ñ–ù–û–°–¢–¨ –ü–†–ò–ó–ù–ê–ö–û–í (—Ñ–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å):")
    print("-"*40)
    
    importance_df = pd.DataFrame({
        'feature': feature_names,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    # –¢–æ–ø-15 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    print("\n–¢–æ–ø-15 –≤–∞–∂–Ω–µ–π—à–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
    for idx, row in importance_df.head(15).iterrows():
        bar = '‚ñà' * int(row['importance'] * 100)
        print(f"{row['feature']:30s}: {row['importance']:.3f} {bar}")
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    plt.figure(figsize=(10, 8))
    top_20 = importance_df.head(20)
    
    plt.barh(range(len(top_20)), top_20['importance'], color='steelblue')
    plt.yticks(range(len(top_20)), top_20['feature'])
    plt.xlabel('Importance')
    plt.title('Top 20 Feature Importance - Final Model')
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.savefig(RESULTS / 'final_feature_importance.png', dpi=100, bbox_inches='tight')
    plt.show()
    
    return importance_df

def create_submission(model, X_test, test_ids):
    """–°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ submission."""
    print("\nüìù –°–û–ó–î–ê–ù–ò–ï SUBMISSION:")
    print("-"*40)
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    if hasattr(model, 'predict'):  # Random Forest
        predictions = model.predict(X_test)
        probabilities = model.predict_proba(X_test)[:, 1]
    else:  # XGBoost
        dtest = xgb.DMatrix(X_test)
        probabilities = model.predict(dtest)
        predictions = (probabilities > 0.5).astype(int)
    
    # –°–æ–∑–¥–∞–µ–º DataFrame
    submission = pd.DataFrame({
        'id': test_ids,
        'loan_status': predictions
    })
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print(f"\nüìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:")
    print(f"   –û—Ç–∫–∞–∑—ã (0): {(predictions == 0).sum()} ({(predictions == 0).mean():.1%})")
    print(f"   –û–¥–æ–±—Ä–µ–Ω–∏—è (1): {(predictions == 1).sum()} ({(predictions == 1).mean():.1%})")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è
    train_ratio = 0.142  # 14.2% –æ–¥–æ–±—Ä–µ–Ω–∏–π –≤ train
    pred_ratio = predictions.mean()
    print(f"\n   Train ratio: {train_ratio:.1%}")
    print(f"   Pred ratio: {pred_ratio:.1%}")
    print(f"   –†–∞–∑–Ω–∏—Ü–∞: {abs(pred_ratio - train_ratio):.1%}")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f'submission_{timestamp}.csv'
    submission.to_csv(SUBMISSIONS / filename, index=False)
    
    # –¢–∞–∫–∂–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –ø–æ—Å–ª–µ–¥–Ω–∏–π
    submission.to_csv(SUBMISSIONS / 'submission_latest.csv', index=False)
    
    print(f"\n‚úÖ Submission —Å–æ—Ö—Ä–∞–Ω–µ–Ω:")
    print(f"   {SUBMISSIONS / filename}")
    print(f"   {SUBMISSIONS / 'submission_latest.csv'}")
    
    return submission

def save_final_model(model, cv_score, feature_importance):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö."""
    print("\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏...")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    with open(MODEL_V3 / 'model.pkl', 'wb') as f:
        pickle.dump(model, f)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
    with open(MODEL_V3 / 'metrics.txt', 'w') as f:
        f.write("FINAL MODEL METRICS\n")
        f.write("="*50 + "\n")
        f.write(f"Model: RandomForestClassifier\n")
        f.write(f"Cross-validation ROC-AUC: {cv_score:.4f}\n")
        f.write(f"Number of features: {len(feature_importance)}\n")
        f.write(f"Training date: {datetime.now()}\n")
        f.write("\nTop 10 features:\n")
        for idx, row in feature_importance.head(10).iterrows():
            f.write(f"  {row['feature']}: {row['importance']:.3f}\n")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    feature_importance.to_csv(MODEL_V3 / 'feature_importance.csv', index=False)
    
    print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {MODEL_V3}")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    train_df, test_df, test_ids = load_data()
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    X_train_full = train_df.drop(['loan_status'], axis=1, errors='ignore')
    y_train_full = train_df['loan_status']
    X_test = test_df
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (–¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π)
    X_train, X_val, y_train, y_val = train_test_split(
        X_train_full, y_train_full, 
        test_size=0.2, 
        random_state=42, 
        stratify=y_train_full
    )
    
    # –û–±—É—á–µ–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
    rf_model, rf_auc = train_random_forest_optimized(X_train, y_train, X_val, y_val)
    
    # XGBoost (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ - –Ω—É–∂–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å: pip install xgboost)
    try:
        xgb_model, xgb_auc = train_xgboost(X_train, y_train, X_val, y_val)
    except:
        print("\n‚ö†Ô∏è XGBoost –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–µ–º Random Forest.")
        xgb_model, xgb_auc = None, 0
    
    # –í—ã–±–æ—Ä –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
    if xgb_auc > rf_auc:
        print(f"\nüèÜ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: XGBoost (AUC={xgb_auc:.4f})")
        best_model_type = 'xgboost'
    else:
        print(f"\nüèÜ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: Random Forest (AUC={rf_auc:.4f})")
        best_model_type = 'rf'
    
    # –û–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –í–°–ï–• –¥–∞–Ω–Ω—ã—Ö
    final_model, cv_score = train_final_model(X_train_full, y_train_full)
    
    # –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏
    feature_importance = analyze_feature_importance(final_model, X_train_full.columns)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ submission
    submission = create_submission(final_model, X_test, test_ids)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    save_final_model(final_model, cv_score, feature_importance)
    
    # –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
    print("\n" + "="*60)
    print("üéâ –ü–†–û–ï–ö–¢ –ó–ê–í–ï–†–®–ï–ù!")
    print("="*60)
    print(f"‚úÖ –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞")
    print(f"‚úÖ Cross-validation AUC: {cv_score:.4f}")
    print(f"‚úÖ Submission —Å–æ–∑–¥–∞–Ω: {len(submission)} –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π")
    print(f"‚úÖ –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
    print("\nüöÄ –ì–æ—Ç–æ–≤–æ –∫ –æ—Ç–ø—Ä–∞–≤–∫–µ –Ω–∞ Kaggle!")

if __name__ == "__main__":
    main()