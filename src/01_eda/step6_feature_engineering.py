"""
===============================================================================
                    STEP 6: FEATURE ENGINEERING
                    
                    –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ —Ñ–∏–Ω–∞–ª—å–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞
===============================================================================

–¶–ï–õ–¨:
-----
1. –£–¥–∞–ª–∏—Ç—å –¥—É–±–ª–∏ (person_income –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ log –≤–µ—Ä—Å–∏—é)
2. –°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è —Å—É–±–ø—Ä–∞–π–º-–ª–æ–≥–∏–∫–∏
3. One-Hot Encoding –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π
4. –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö —á–∏—Å–ª–æ–≤—ã—Ö
5. –ó–∞–ø—É—Å—Ç–∏—Ç—å baseline v2 –∏ –∏–∑–º–µ—Ä–∏—Ç—å —É–ª—É—á—à–µ–Ω–∏–µ

–û–ñ–ò–î–ê–ï–ú–´–ô –†–ï–ó–£–õ–¨–¢–ê–¢:
-------------------
- 20-25 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤–º–µ—Å—Ç–æ 11
- ROC-AUC —É–ª—É—á—à–µ–Ω–∏–µ –¥–æ 0.89-0.90
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import warnings
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score, classification_report
import pickle

warnings.filterwarnings('ignore')

# –ü—É—Ç–∏
ROOT_DIR = Path(__file__).parent.parent.parent
DATA_PROCESSED = ROOT_DIR / 'data' / 'processed'
DATA_ENGINEERED = ROOT_DIR / 'data' / 'engineered'
RESULTS = ROOT_DIR / 'results' / 'step6_engineering'
MODEL_V2 = ROOT_DIR / 'results' / 'model_versions' / 'v2_after_engineering'

# –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫–∏
DATA_ENGINEERED.mkdir(parents=True, exist_ok=True)
RESULTS.mkdir(parents=True, exist_ok=True)
MODEL_V2.mkdir(parents=True, exist_ok=True)

def load_data():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –æ—á–∏—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö."""
    print("\n" + "="*60)
    print("STEP 6: FEATURE ENGINEERING")
    print("="*60)
    
    train_df = pd.read_csv(DATA_PROCESSED / 'train_cleaned.csv')
    test_df = pd.read_csv(DATA_PROCESSED / 'test_cleaned.csv')
    
    print(f"\nüìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ:")
    print(f"   Train: {len(train_df)} –∑–∞–ø–∏—Å–µ–π, {len(train_df.columns)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    print(f"   Test: {len(test_df)} –∑–∞–ø–∏—Å–µ–π")
    
    return train_df, test_df

def remove_duplicates(df):
    """–£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤."""
    print("\nüîß –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–µ–π:")
    print("-"*40)
    
    # –£–¥–∞–ª—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π person_income (–æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ log –≤–µ—Ä—Å–∏—é)
    if 'person_income' in df.columns and 'person_income_log' in df.columns:
        df = df.drop('person_income', axis=1)
        print("   ‚úÖ –£–¥–∞–ª–µ–Ω person_income (–æ—Å—Ç–∞–≤–ª–µ–Ω person_income_log)")
    
    return df

def create_subprime_features(df):
    """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è —Å—É–±–ø—Ä–∞–π–º-–ª–æ–≥–∏–∫–∏."""
    print("\nüöÄ –°–æ–∑–¥–∞–Ω–∏–µ –ù–û–í–´–• –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
    print("-"*40)
    
    # 1. High Risk Score - –∫–æ–º–±–∏–Ω–∞—Ü–∏—è –ø–ª–æ—Ö–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    df['high_risk_score'] = (
        (df['loan_grade'].isin(['F', 'G'])).astype(int) +
        (df['cb_person_default_on_file'] == 'Y').astype(int) +
        (df['person_home_ownership'] == 'RENT').astype(int) +
        (df['loan_percent_income'] > 0.25).astype(int)
    )
    print("   ‚úÖ high_risk_score: —Å—É–º–º–∞ —Ä–∏—Å–∫–æ–≤—ã—Ö —Ñ–∞–∫—Ç–æ—Ä–æ–≤ (0-4)")
    
    # 2. Is Subprime Client - –∏–¥–µ–∞–ª—å–Ω—ã–π –∫–ª–∏–µ–Ω—Ç –¥–ª—è —Å—É–±–ø—Ä–∞–π–º–∞
    df['is_subprime_client'] = (
        (df['loan_grade'].isin(['D', 'E', 'F', 'G'])) & 
        (df['loan_percent_income'] > 0.20)
    ).astype(int)
    print("   ‚úÖ is_subprime_client: –ø–ª–æ—Ö–æ–π –≥—Ä–µ–π–¥ + –≤—ã—Å–æ–∫–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞")
    
    # 3. DTI –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (–±–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω—ã–µ)
    df['dti_critical'] = (df['loan_percent_income'] > 0.35).astype(int)
    df['dti_high'] = (df['loan_percent_income'] > 0.25).astype(int)
    print("   ‚úÖ dti_critical/high: –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –¥–æ–ª–≥–æ–≤–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞")
    
    # 4. Age –≥—Ä—É–ø–ø—ã –¥–ª—è —Å—É–±–ø—Ä–∞–π–º–∞
    df['age_risk_young'] = (df['person_age'] < 25).astype(int)
    df['age_risk_old'] = (df['person_age'] > 60).astype(int)
    print("   ‚úÖ age_risk: –º–æ–ª–æ–¥—ã–µ –∏ –ø–æ–∂–∏–ª—ã–µ")
    
    # 5. Income to loan ratio
    df['income_loan_ratio'] = df['person_income_log'] / (np.log1p(df['loan_amnt']) + 1)
    print("   ‚úÖ income_loan_ratio: –æ—Ç–Ω–æ—à–µ–Ω–∏–µ –¥–æ—Ö–æ–¥–∞ –∫ –∫—Ä–µ–¥–∏—Ç—É")
    
    # 6. Bad grade with high rate
    df['bad_grade_high_rate'] = (
        (df['loan_grade'].isin(['E', 'F', 'G'])) & 
        (df['loan_int_rate'] > 15)
    ).astype(int)
    print("   ‚úÖ bad_grade_high_rate: –ø–ª–æ—Ö–æ–π –≥—Ä–µ–π–¥ + –≤—ã—Å–æ–∫–∞—è —Å—Ç–∞–≤–∫–∞")
    
    # 7. –ö–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è –¥–æ–ª–≥–æ–≤ —Å –ø–ª–æ—Ö–∏–º –≥—Ä–µ–π–¥–æ–º
    df['debt_consolidation_risk'] = (
        (df['loan_intent'] == 'DEBTCONSOLIDATION') & 
        (df['loan_grade'].isin(['D', 'E', 'F', 'G']))
    ).astype(int)
    print("   ‚úÖ debt_consolidation_risk: –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è + –ø–ª–æ—Ö–æ–π –≥—Ä–µ–π–¥")
    
    # 8. –ö–æ—Ä–æ—Ç–∫–∞—è –∫—Ä–µ–¥–∏—Ç–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è —Å –≤—ã—Å–æ–∫–∏–º —Ä–∏—Å–∫–æ–º
    df['short_history_risk'] = (
        (df['cb_person_cred_hist_length'] < 3) & 
        (df['loan_grade'].isin(['F', 'G']))
    ).astype(int)
    print("   ‚úÖ short_history_risk: –º–∞–ª–æ –∏—Å—Ç–æ—Ä–∏–∏ + –ø–ª–æ—Ö–æ–π –≥—Ä–µ–π–¥")
    
    print(f"\n   üéØ –°–æ–∑–¥–∞–Ω–æ 10 –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤!")
    
    return df

def encode_categorical(df, is_train=True):
    """One-Hot Encoding –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤."""
    print("\nüîß –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö:")
    print("-"*40)
    
    categorical_cols = ['person_home_ownership', 'loan_intent', 'loan_grade']
    
    for col in categorical_cols:
        # One-hot encoding
        dummies = pd.get_dummies(df[col], prefix=col, drop_first=False)
        df = pd.concat([df, dummies], axis=1)
        df = df.drop(col, axis=1)
        print(f"   ‚úÖ {col}: —Å–æ–∑–¥–∞–Ω–æ {len(dummies.columns)} –±–∏–Ω–∞—Ä–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    
    # cb_person_default_on_file - –ø—Ä–æ—Å—Ç–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ (Y=1, N=0)
    if 'cb_person_default_on_file' in df.columns:
        df['cb_person_default_on_file'] = (df['cb_person_default_on_file'] == 'Y').astype(int)
        print("   ‚úÖ cb_person_default_on_file: Y‚Üí1, N‚Üí0")
    
    return df

def scale_features(train_df, test_df):
    """–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤."""
    print("\nüìè –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
    print("-"*40)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —á–∏—Å–ª–æ–≤—ã–µ —Å—Ç–æ–ª–±—Ü—ã (–∏—Å–∫–ª—é—á–∞–µ–º –±–∏–Ω–∞—Ä–Ω—ã–µ –∏ —Ç–∞—Ä–≥–µ—Ç)
    numeric_cols = [
        'person_age', 'person_income_log', 'person_emp_length',
        'loan_amnt', 'loan_int_rate', 'loan_percent_income',
        'cb_person_cred_hist_length', 'income_loan_ratio'
    ]
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ
    numeric_cols = [col for col in numeric_cols if col in train_df.columns]
    
    # –°–æ–∑–¥–∞–µ–º –∏ –æ–±—É—á–∞–µ–º StandardScaler
    scaler = StandardScaler()
    
    # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º train
    train_df[numeric_cols] = scaler.fit_transform(train_df[numeric_cols])
    
    # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º test (–∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ—Ç train!)
    test_df[numeric_cols] = scaler.transform(test_df[numeric_cols])
    
    print(f"   ‚úÖ –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–æ {len(numeric_cols)} —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    print(f"   –ú–µ—Ç–æ–¥: StandardScaler (mean=0, std=1)")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º scaler
    with open(DATA_ENGINEERED / 'scaler.pkl', 'wb') as f:
        pickle.dump(scaler, f)
    
    return train_df, test_df, scaler

def run_baseline_v2(train_df):
    """–ó–∞–ø—É—Å–∫ baseline –Ω–∞ engineered –¥–∞–Ω–Ω—ã—Ö."""
    print("\n" + "="*60)
    print("BASELINE MODEL v2 - After Feature Engineering")
    print("="*60)
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    X = train_df.drop(['id', 'loan_status'], axis=1, errors='ignore')
    y = train_df['loan_status']
    
    print(f"\nüìä –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö: {X.shape}")
    print(f"   –ë—ã–ª–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: 11")
    print(f"   –°—Ç–∞–ª–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X.shape[1]}")
    
    # Train/Val split
    X_train, X_val, y_train, y_val = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    # ========== LOGISTIC REGRESSION ==========
    print("\nüîµ Logistic Regression:")
    lr_model = LogisticRegression(
        class_weight='balanced',
        max_iter=1000,
        random_state=42
    )
    lr_model.fit(X_train, y_train)
    
    lr_pred_proba = lr_model.predict_proba(X_val)[:, 1]
    lr_auc = roc_auc_score(y_val, lr_pred_proba)
    print(f"   ROC-AUC: {lr_auc:.4f}")
    
    # ========== RANDOM FOREST ==========
    print("\nüå≤ Random Forest:")
    rf_model = RandomForestClassifier(
        n_estimators=100,
        max_depth=10,
        class_weight='balanced',
        random_state=42,
        n_jobs=-1
    )
    rf_model.fit(X_train, y_train)
    
    rf_pred_proba = rf_model.predict_proba(X_val)[:, 1]
    rf_auc = roc_auc_score(y_val, rf_pred_proba)
    print(f"   ROC-AUC: {rf_auc:.4f}")
    
    # ========== –°–†–ê–í–ù–ï–ù–ò–ï ==========
    print("\nüìä –°–†–ê–í–ù–ï–ù–ò–ï –° –ü–†–ï–î–´–î–£–©–ò–ú–ò –í–ï–†–°–ò–Ø–ú–ò:")
    print("-"*50)
    
    baseline_v0 = 0.8740  # –ò—Å—Ö–æ–¥–Ω—ã–π baseline
    baseline_v1 = 0.8730  # –ü–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏
    
    print(f"   v0_baseline (—Å—ã—Ä—ã–µ):     {baseline_v0:.4f}")
    print(f"   v1_cleaned:              {baseline_v1:.4f}")
    print(f"   v2_LR (engineered):      {lr_auc:.4f} ({lr_auc-baseline_v0:+.4f})")
    print(f"   v2_RF (engineered):      {rf_auc:.4f} ({rf_auc-baseline_v0:+.4f}) üî•")
    
    # ========== –í–ê–ñ–ù–û–°–¢–¨ –ù–û–í–´–• –ü–†–ò–ó–ù–ê–ö–û–í ==========
    print("\nüéØ –¢–æ–ø-10 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (Random Forest):")
    print("-"*40)
    
    feature_importance = pd.DataFrame({
        'feature': X.columns,
        'importance': rf_model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    for idx, row in feature_importance.head(10).iterrows():
        bar = '‚ñà' * int(row['importance'] * 50)
        print(f"{row['feature']:30s}: {row['importance']:.3f} {bar}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–ø–∞–ª–∏ –ª–∏ –Ω–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ —Ç–æ–ø
    new_features = ['high_risk_score', 'is_subprime_client', 'dti_critical', 
                   'bad_grade_high_rate', 'debt_consolidation_risk']
    new_in_top = feature_importance.head(15)['feature'].isin(new_features).sum()
    print(f"\n‚ú® –ù–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ —Ç–æ–ø-15: {new_in_top} –∏–∑ {len(new_features)}")
    
    # ========== –°–û–•–†–ê–ù–ï–ù–ò–ï ==========
    with open(MODEL_V2 / 'metrics.txt', 'w') as f:
        f.write("BASELINE MODEL v2 - After Feature Engineering\n")
        f.write("="*50 + "\n")
        f.write(f"Logistic Regression ROC-AUC: {lr_auc:.4f}\n")
        f.write(f"Random Forest ROC-AUC: {rf_auc:.4f}\n")
        f.write(f"Improvement from v0: {rf_auc-baseline_v0:+.4f}\n")
        f.write(f"\nTotal features: {X.shape[1]}\n")
        f.write("\nNew features created:\n")
        for feat in new_features:
            f.write(f"- {feat}\n")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
    best_model = rf_model if rf_auc > lr_auc else lr_model
    with open(MODEL_V2 / 'model.pkl', 'wb') as f:
        pickle.dump(best_model, f)
    
    return rf_auc, feature_importance

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    
    # –ó–∞–≥—Ä—É–∑–∫–∞
    train_df, test_df = load_data()
    
    # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–µ–π
    train_df = remove_duplicates(train_df)
    test_df = remove_duplicates(test_df)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    train_df = create_subprime_features(train_df)
    test_df = create_subprime_features(test_df)
    
    # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö
    train_df = encode_categorical(train_df, is_train=True)
    test_df = encode_categorical(test_df, is_train=False)
    
    # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
    train_df, test_df, scaler = scale_features(train_df, test_df)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ engineered –¥–∞–Ω–Ω—ã—Ö
    train_df.to_csv(DATA_ENGINEERED / 'train_engineered.csv', index=False)
    test_df.to_csv(DATA_ENGINEERED / 'test_engineered.csv', index=False)
    print(f"\nüíæ Engineered –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {DATA_ENGINEERED}")
    
    # –ó–∞–ø—É—Å–∫ baseline v2
    rf_auc, feature_importance = run_baseline_v2(train_df)
    
    # –ò—Ç–æ–≥–∏
    print("\n" + "="*60)
    print("STEP 6 COMPLETED!")
    print("="*60)
    print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ 10 –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    print(f"‚úÖ –í—Å–µ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(train_df.columns)-2}")
    print(f"‚úÖ Random Forest ROC-AUC: {rf_auc:.4f}")
    print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ {MODEL_V2}")

if __name__ == "__main__":
    main()